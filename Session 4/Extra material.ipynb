{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da420e9e",
   "metadata": {},
   "source": [
    "### Gradient descent optimizers\n",
    "\n",
    "* **Stochastic gradient descent**: A single training example is used in each iteration. The gradient (or the derivative of the cost function) is computed for that training example and it is used to update the weights.\n",
    "* **Batch gradient descent**: The entire training set is used in each iteration. The average of the gradients for all the training examples is computed and it is used to update the weights.\n",
    "\n",
    "Notes:\n",
    "* The stochastic converges much faster for larger datasets than the batch gradient descent since the weights are updated a lot more frequently.\n",
    "* For batch gradient descent, the cost function declines consistently with each iteration, whereas for the stochastic gradient descent, the cost fluctuates and declines overall after each epoch.\n",
    "* The stochastic gradient descent cannot make use of the vectorized operations unlike batch gradient descent.\n",
    "\n",
    "\n",
    "**Mini-batch gradient descent** is a mix of the above two and a good compromise. The training set is divided into batches and a single batch is used in each iteration.  The batch sizes of 32, 64 and 128 are often used.\n",
    "\n",
    "In practice, mini-batch is most commonly used, especially for the large datasets. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1634/1*PV-fcUsNlD9EgTIc61h-Ig.png\" width=600 />\n",
    "\n",
    "\n",
    "### Tuning the learning rate \n",
    "\n",
    "The learning rate ($\\alpha$), that determines the size of the steps in the gradient descent algorithm, is an important hyperparameter that needs to be tuned. \n",
    "* If the learning rate is too low, then it takes too long to converge. \n",
    "* If the learning rate is too high, then it might oscillate and never reach the minima.\n",
    "\n",
    "$$ w := w - \\alpha \\frac{\\partial J}{\\partial w}$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*9zqj3nwIEU-L0-9pYitcRA.png\" width=600 />\n",
    "\n",
    "Our intuition tells us that the learning rate $\\alpha$ should be larger at the beginning when are weights are initialized randomly and are far from being optimal and then it can be reduced as the training process proceeds.This is called learning rate schedule or learning rate decay. The two common methods used are linear decay or exponential decay.\n",
    " \n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/nn3/learningrates.jpeg\" width=300 />\n",
    "\n",
    "\n",
    "The tuning of the learning rate has been an area of research and the adaptive gradient descent algorithms such as Adam, RMSprop, AdaGrad, etc. are developed that usually give better results.\n",
    "\n",
    "### Normalizing the features:\n",
    "\n",
    "Suppose, we have only two features $x_1$ and $x_2$. Let us also suppose that $x_1$ takes values in the range of $0$ to $0.1$ whereas $x_2$ takes values in the range of $100$ to $500$. How do you think the difference in the scales of the two features will affect the training process of the network?\n",
    "\n",
    "If the scales of the features vary a lot, the gradient descent takes a longer time to converges. It is helpful to normalize the features to be in the range of $-1$ to $1$ to speed up the learning process. \n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2018-01-23-at-2.27.20-PM.png\" width=600 />\n",
    "\n",
    "If the input features are not in the typical range of $-1$ to $1$, the default parameters for the network such as learning rate can also not work properly for the training process. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
