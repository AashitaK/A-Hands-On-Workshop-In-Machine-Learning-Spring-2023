{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b961d7f",
   "metadata": {},
   "source": [
    "### Revision:\n",
    "Let us summarize what we have learned so far.\n",
    "\n",
    "Some common terms:\n",
    "* Multi-layer Perceptron (MLP)\n",
    "* Nodes\n",
    "* Input layer\n",
    "* Hidden layers\n",
    "* Output layer\n",
    "* Weights\n",
    "* Bias\n",
    "* Weighted sum $z_i$\n",
    "* Activation functions $g_i$\n",
    "    * sigmoid function (also known as logistic function)\n",
    "* Activations $a_i$\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png\" width=\"300\" height=\"350\" />\n",
    "\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=1kcWsASHFLoEgRFNpi_cxgYUElzUvOYro\" width=600 />\n",
    "\n",
    "Some points to note about neural network architecture:\n",
    "* The number of nodes in the input layer is equal to the number of features plus the bias. \n",
    "* Each layer except the output layer has a bias, though they are often not shown in the neural network diagrams. \n",
    "    * The bias has no incoming connection from the nodes in the previous layer, unlike other nodes in the hidden layers\n",
    "    * The bias is connected to each node in the next layer, just like other nodes.\n",
    "* The output/activation of each layer is calculated in two steps: weighted sum of the incoming input followed by the activation function.\n",
    "$$z_k = a_{k-1}*W_k+b_k$$\n",
    "$$a_k = g_k(z_k)$$\n",
    "* The output/activation of each hidden layer becomes the input of the next layer.\n",
    "* All the nodes in a layer share the same activation function but the activation functions can differ from layer to layer.\n",
    "* The activation functions contributes to the non-linearity in the model. \n",
    "* The weights and bias of a neural network are learned using the training examples.\n",
    "\n",
    "Equations:  \n",
    "$$z_1 = x*W_1+b_1$$\n",
    "$$a_1 = g_1(z_1)$$\n",
    "$$z_2 = a_1*W_2+b_2$$\n",
    "$$a_2 = g_2(z_2)$$\n",
    "$$ \\vdots $$\n",
    "$$z_n = a_{n-1}*W_n+b_n$$\n",
    "$$a_n = g_n(z_n)$$\n",
    "and so on till the final output $y_{pred} = a_n$.\n",
    "\n",
    "More terms:\n",
    "* Cost function $J$\n",
    "* Gradient descent algorithm\n",
    "* Forward propagation\n",
    "* Backward propagation\n",
    "* Iterations vs epochs\n",
    "* Learning rate $\\alpha$\n",
    "\n",
    "The iterative training process in a nutshell: \n",
    "1. The weights are initialized.\n",
    "2. For each input vector, the activations are propagated forward thru the network to give the final output. This is **forward propagation**.\n",
    "3. This final output is compared with the target value to calculate the cost function.\n",
    "4. The above cost is propagated backwards using gradients. This is called **backpropagation**. \n",
    "5. The weights are updated using the gradients calculated above.\n",
    "\n",
    "There are different choices of cost functions that are used for the neural networks. The commonly used cost functions are given below. \n",
    "\n",
    "Mean-squared Error (MSE) for regression:\n",
    "$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 $$\n",
    "\n",
    "Mean-absolute Error (MSE) for regression:\n",
    "$$ J = \\frac{1}{n} \\sum_{i=1}^n |y^{(i)} - y_{pred}^{(i)}| $$\n",
    "\n",
    "Log-loss/Cross-entropy for binary classification:\n",
    "$$ J = - \\frac{1}{n} \\sum_{i=1}^n y \\log(p) + (1-y) \\log(1-p)$$\n",
    "\n",
    "Many common tasks for supervised machine learning can be formulated as:\n",
    "- Classification\n",
    "    - Binary classification - positive and negative classes, decision boundary\n",
    "- Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a6fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
