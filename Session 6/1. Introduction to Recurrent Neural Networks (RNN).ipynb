{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Hands-on Workshop series in Machine Learning\n",
    "#### Instructor: Dr. Aashita Kesarwani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "In the previous class, we learned a couple of ways to convert the text data samples into numerical vectors. We used the Bag-Of-Words (BOW) method for vectorization of input sentences.\n",
    "* The input/feature vectors were of the length of the pre-defined vocabulary \n",
    "* Either simple frequencies (CountVectorizer) or TF-IDF frequencies (TfidfVectorizer) was used for converting input sentences to numerical feature vectors. \n",
    "\n",
    "TF-IDF frequencies are obtained by multiplying term frequencies (TF) with inverse document frequences (IDF). \n",
    "\n",
    "Term frequences (TF) are simply counts whereas the inverse document frequences (IDF) give more weighs to uncommon rare words and less weigh to common words.\n",
    "\n",
    "What are the drawbacks of the above two Bag-Of-Words (BOW) methods?\n",
    "\n",
    "* **High dimension for input vectors**: If we use feedforward neural networks, the number of input nodes is too large, so consequently the nodes in subsequent layers would need to be proportionally large for not losing a lot of information. This would make training process computationally expensive for larger datasets.\n",
    "* **Loss of order of words**: The ordering of words in the input sentences is completely lost. We know that a lot of information in our languages is encoded in the order of words, so we should think of using an architecture that would take it into account.\n",
    "\n",
    "We know that a lot of information in our languages is encoded in the order of words, so we should think of using an architecture that would take it into account.\n",
    "\n",
    "Can you think of ways to vectorize sentences while preserving the order of words?\n",
    "\n",
    "An easiest way to preserve the order or words while vectorization would be to\n",
    "* Set a certain length for input sequences, say 20 words\n",
    "* Process all the sentences in the training dataset so they consist of exactly $n$ words (say $20$ words) either by padding with null words or chopping off longer sentences\n",
    "* Create a vocabulary of all words in the training data and assign them hashes (numbers)\n",
    "* Vectorize the input sequences by replacing the words with their hashes in the vocabulary\n",
    "* Feed this input to a vanilla neural network\n",
    "\n",
    "\n",
    "The feed forward neural network with the above input might be able to capture information about the order of words in the sentence but it would need to learn all rules of the language separately at each point. For example, it would treat the following two sentences differently \n",
    "* \"I went to Hawaii in 2018.\"\n",
    "* \"In 2018, I went to Hawaii.\"\n",
    "\n",
    "The relevant information about when the narrator went to Hawaii is present in 6th and 2nd position respectively. Many more training examples would be needed to train the network to extract the relevant information as the network needs to learn the same rules separately for all positions in the input layer.\n",
    "\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are specifically designed to work well on sequential data. They are much more efficient for textual data as compared to vanilla neural network (i.e. multi-layer perceptrons).\n",
    "\n",
    "Other than natural language processing, RNNs can be applied to time-series based data as well. \n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "\n",
    "* RNN takes an input of sequence, denoted as $x^{(1)}, x^{(2)}, \\dots, x^{(t)}, \\dots$, instead of a single input vector as seen previously.\n",
    "* It can handle input sequences of any length.\n",
    "* The information from the time step $t-1$ is passed on as input to the next time step $t$.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1w7enotgoPLfKKsNV-1-jZxciBkMDws9z\" width=\"800\" height=\"200\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematics, we often study dynamical systems that involves recurrence relations:\n",
    "\n",
    "$$s^{(t)} = f(s^{(t-1)}; \\theta)$$\n",
    "\n",
    "where $s^{(t)}$ is the state of system at time $t$.\n",
    "\n",
    "These dynamical systems can also involve input signals $x^{(t)}$ at each step:\n",
    "\n",
    "$$s^{(t)} = f(s^{(t-1)}, x^{(t)} ; \\theta)$$\n",
    "\n",
    "Such a recurrence relation is involved in the architecture of most RNN (though the architecture can vary greatly as discussed below). The hidden nodes in RNN at time $t$ can be defined as:\n",
    "\n",
    "$$h^{(t)} = f(h^{(t-1)}, x^{(t)} ; \\theta)$$\n",
    "\n",
    "Here, the hidden state $h^{(t)}$ defined in terms of $h^{(t-1)}$ can be unfolded in the following manner, for say $t=3$:\n",
    "\n",
    "\\begin{align}\n",
    " h^{(3)} &= f(h^{(2)}, x^{(3)}; \\theta) \\\\ \n",
    " &= f(f(h^{(1)}, x^{(2)}; \\theta), x^{(3)}; \\theta) \\\\\n",
    " &= f(f(f(h^{(0)}, x^{(1)}; \\theta), x^{(2)}; \\theta), x^{(3)}; \\theta) \\\\\n",
    " &= g^{(3)}(x^{(1)}, x^{(2)}, x^{(3)})\n",
    "\\end{align}\n",
    "\n",
    "Thus, the unfolded recurrence can also be represented by a function $g^{(t)}$ as:\n",
    "\n",
    "\\begin{align}\n",
    " h^{(t)} &= f(h^{(t-1)}, x^{(t)}; \\theta) \\\\ \n",
    " &= g^{(t)}(x^{(1)}, \\dots, x^{(t-1)}, x^{(t)})\n",
    "\\end{align}\n",
    "\n",
    "Advantages of using the model $h^{(t)} = f(h^{(t-1)}, x^{(t)} ; \\theta)$ over $h^{(t)} = g^{(t)}(x^{(1)}, \\dots, x^{(t-1)}, x^{(t)})$:\n",
    "* The function $g^{(t)}$ can be a different function at each time step but the ***same*** transition function $f$ with the same parameters $\\theta$ can be used to define hidden state in terms of $h^{(t-1)}$ and input $x^{(t)}$ for every time step. \n",
    "* The input sequences can be of variable length but the input for the model is always the same size consisting of $h^{(t-1)}$ and input $x^{(t)}$.\n",
    "\n",
    "Thus, a single shared model $f$ can be learned that will generalize to sequences of any length and will eliminate the need to learn a separate model $g^{(t)}$ for each time step. This parameter sharing also allows us to train the model with far fewer training examples.\n",
    "\n",
    "As the RNN involve function compositions multiple times, the composite function can result in very high non-linear behaviour.\n",
    "\n",
    "Some common types of RNN architecture:\n",
    "\n",
    "### 1. Output at each step and recurrent connections between hidden units \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gnZmLW2WEkqDakNnGxy0OljT9NWBUFsD\" width=\"600\" />\n",
    "\n",
    "\n",
    "The equations for the simple RNN architecture:\n",
    "\n",
    "\\begin{align}\n",
    " a^{(t)} &= b + Wh^{(t-1)} + U x^{(t)} \\\\ \n",
    " h^{(t)} &= \\tanh (a^{(t)})\\\\\n",
    " o^{(t)} &= c + Vh^{(t)}\\\\\n",
    " \\hat{y}^{(t)} &= \\text{softmax} (o^{(t)})\\\\\n",
    "\\end{align}\n",
    "\n",
    "The activation function for the hidden unit is hyperbolic tangent which is simply sigmoid function scaled and translated along y-axis.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/980px-Hyperbolic_Tangent.svg.png?20090905154026\" width=\"450\" height=\"200\" />\n",
    "\n",
    "Cost function, $L$, is the sum of negative loglikelihood of $y^{(t)}$ given $x^{(1)}, \\dots, x^{(t-1)}, x^{(t)}$\n",
    "\n",
    "$$ L = \\sum_t L^{(t)} = - \\sum_t \\log p \\big(y^{(t)} | x^{(1)}, \\dots, x^{(t-1)}, x^{(t)} \\big)$$\n",
    "\n",
    "### 2. Output at each step and recurrent connections from output at one time step to hidden units at the next\n",
    "\n",
    "* A slight modification of the above architecture where the recurrent connection goes from **output at one time step to hidden units at the next** instead of connecting the subsequent hidden units. \n",
    "\n",
    "##### Drawback\n",
    "The output is the only information that is allowed to pass on to the next unit in this architecture. We know that the output is optimized for correct prediction instead of passing on the relevant background information to the next time step. \n",
    "##### Advantage\n",
    "This RNN architecture is far easier to train and allows parallelization using teacher forcing explained below.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1H6t2P0Civ_DI0qfaUjLx-Ua9bY57R_vX\" width=\"600\" />\n",
    "\n",
    "### 3. A single output at the end and recurrent connections between hidden units\n",
    "When the output is only needed at the end after reading the entire input sequence, this architecture is used.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1vkmFkE94zgkHXfiqrpcExo5HKikevoWW\" width=\"600\" />\n",
    "\n",
    "\n",
    "Some applications for a single output:\n",
    "* Sentiment analysis (Text classification)\n",
    "* Sentence completion (guessing the next word in the sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Back-propagation through time (BPTT):\n",
    "The forward propagation moves from left-to-right along the time steps whereas the backpropagation moves in the opposite direction from right-to-left. The computations for weight updates in the backpropagation cannot be parallelized for different time steps as the forward propagation is sequential and hence, the computations cannot be done for time step $t=\\tau$ unless they are done for all time steps upto $t=\\tau-1$. Thus, the RNNs can be powerful but very expensive to train.\n",
    "\n",
    "####  Teacher forcing\n",
    "The model of the second type that have recurrent connections from output at one time step to hidden units at the next can be trained using teacher forcing. Instead of using the predicted output from the previous time step, the true/targeted output is used. This makes it possible to parallelize the backpropagation as each time step can be conssidered as an independent unit. It is no longer necessary that the computations are completed for the previous time steps before calculation the gradient for the current one. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12XXv40NXjs9iOUhMoRdZkiqGbMwmBsHc\" width=\"400\" height=\"70\" />\n",
    "\n",
    "While making predictions on unseen data using the tranined RNN, the predicted output from the previous time step is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The challenge of Long-term dependencies\n",
    "\n",
    "When we read and try to understand a sentence, we do not only pay attention to the current word, but we also keep the context in mind from what we read previously in the text. This dependence on previous words to understand the context for the current word is called the dependency. The RNN can learn short-term dependencies much better than long-term dependencies, meaning it remembers the context from the neighbouring previous words better but keep losing the information as we move along the sentence.\n",
    "\n",
    "Simple RNNs \n",
    "* Pros:\n",
    "    * Order/sequence of words are considered\n",
    "    * Input can be of any length\n",
    "    * RNNs were unreasonably far more effective for textual data than BOW methods in use at that time (See  [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy). RNN architectures were developed and studied since mid-1980s but their application in NLP tasks did not pick up pace until the computational power and libraries became widely accessible around 2015.\n",
    "* Cons:\n",
    "    1. Suffer from vanishing gradients, especially for longer sequences.\n",
    "    2. Not sufficiently good at long term memory in sequences.\n",
    "    3. Assume one-to-one correspondence between input and output sequences and hence, the architecture is not suitable for many common NLP tasks such as translation between languages, text summarization, question-answering, etc.\n",
    "    4. Slow to train as the training is sequential and thus, cannot be sufficiently parallelized (think in terms of being able to write vectorized code for forward and backward propagation through time steps).\n",
    "    5. Transfer learning is not very useful in application.\n",
    "\n",
    "**Transfer learning**: Using pre-trained neural networks (that were trained on a larger dataset requiring more computing resources) and fine-tuning them for a specific NLP task. The use of transfer learning makes state-of-the-art models widely accessible for users.\n",
    "\n",
    "\n",
    "LSTM and GRU are slight modifications to simple RNN to address the problem of long-term dependencies. Please read [this illustrative article](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) if you learn more about LSTM. They improve the computational time and performance to some extent. \n",
    "\n",
    "\n",
    "### Encoder-decoder or sequence-to-sequence models\n",
    "\n",
    "Simple RNN architectures assumes a correspondence between each input and output sequence at each time step. For many common NLP tasks, the input and output sequences need not be of the same size and a one-to-one correspondence between input and output may be absent. For this purpose, sequence-to-sequence or encoder-decoder architecture is introduced. \n",
    "\n",
    "\n",
    "The encoder creates a numerical context vector from the entire input sequence and pass it on to the decoder to generate the output. \n",
    "\n",
    "* Encoder RNN reads input sequence, called “context” and produces a representation of the context, say C \n",
    "* Final hidden state of encoder RNN is used to compute the (generally) fixed-length context variable C, which represents a semantic summary of the input sequence\n",
    "* Context variable C is Input to the Decoder RNN\n",
    "* Decoder RNN generates output sequence \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*1JcHGUU7rFgtXC_mydUA_Q.jpeg\" width=\"500\" height=\"70\" />\n",
    "\n",
    "The encoder and decoder can also be LSTM/GRU units instead of simple RNN units. The attention-based encoder-decoder or sequence-to-sequence models, also known as transformers, have different architecture involving an attention layer.\n",
    "\n",
    "Encoder-decoder (or sequence-to-sequence models) can be used for several NLP tasks:\n",
    "* Text generation\n",
    "* Text summarization\n",
    "* Question-Answering\n",
    "* Neural machine translation\n",
    "* Audio captioning (speech-to-text)\n",
    "* Text-to-speech conversion\n",
    "* Image captioning\n",
    "\n",
    "### Attention mechanism\n",
    "\n",
    "The context vector creates a bottleneck in passing relevant information for longer sequences. Encoder-decoder models performs a lot better while using attention mechanism which simply passes on more information from each encoder unit to each decoder unit.\n",
    "\n",
    "<img src=\"https://blog.floydhub.com/content/images/2019/09/Slide37-1.JPG\" width=\"750\" />\n",
    "<h4 align=\"center\">\n",
    "Sequence-to-sequence without using attention mechanism \n",
    "</h4>\n",
    "\n",
    "The breakthrough in the performance of deep learning for NLP came from the introduction of transformer models that use attention mechanism (from the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper) instead of recurrent connections across time steps. Transformer based models such as GPT, GPT-2, and GPT-3 by OpenAI, Google's BERT and many modifications of these algorithms have shown remarkable results in most NLP tasks. Transfer learning can be used for fine-tuning pre-trained transformer models freely available online and applying them for various specific tasks in several different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Bag-Of-Words </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Simple RNN </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> LSTM/GRU </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Encoder-Decoder with RNNs </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Encoder-Decoder with RNNs and attention mechanism </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Transformers </center>\n",
    "\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Transformer models consist of encoders and/or decoders. \n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "In general, there are three kinds of transformer models:\n",
    "* *Auto-encoding*: Transformer models such as BERT (Bidirectional Encoder Representations from Transformers), introduced in the [paper](https://arxiv.org/pdf/1810.04805.pdf) by Devlin et al. from Google AI Language in 2019, consists of **only encoders**. Pre-trained BERT model is widely used as \"contextualized word embeddings\". This word embeddings when paired with another architecture such as Multi-Layer Perceptron or LSTM can be used for text classifications problems that we have seen earlier. \n",
    "* *Auto-regressive*: Transformers such as GPT-1/2/3/4(Generative Pre-trained Transformers series) that are language models used for text generation consists of **only decoders**.\n",
    "* *Sequence-to-sequence*: Transformers such as BART, a denoising autoencoder for pretraining sequence-to-sequence models, consists of **both encoders and decoders** and can be used for several NLP tasks such as question-answering, text summarization, etc.\n",
    "\n",
    "\n",
    "<img src=\"https://amatriain.net/blog/images/02-06.png\" width=\"600\" />\n",
    "<h4 align=\"center\">\n",
    "Timeline for Transformer models  \n",
    "</h4>\n",
    "\n",
    "(Image credit: [Xavier Amatriain](https://arxiv.org/pdf/2302.07730.pdf))\n",
    "\n",
    "The original transformer model in the paper [Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) was trained on WMT 2014 English-to-German translation dataset consisting of  4.5 million pairs of phrases. We will refer to this sequence-to-sequence model as vanilla transformer. \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/llm-reading-list/transformer.png\" width=\"650\" />\n",
    "\n",
    "The above image shows a single encoder block and a single decoder block. In the original architecture, the encoder/decoder blocks are stacked on top of each other. Six blocks are used in the model trained in the paper but they can be any number. Though the six encoder blocks have the same structure, their weights are different that will be determined via the training process. The same is true for decoder blocks.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" width=\"600\" />\n",
    "\n",
    "#### Encoder architecture\n",
    "<img  src=\"https://github.com/christianversloot/machine-learning-articles/raw/main/images/Diagram-3.png\" width=\"350\" />\n",
    "\n",
    "The encoder consists of \n",
    "* Input embedding\n",
    "* Positional embedding\n",
    "* Multi-head self-attention\n",
    "* Feedforward network\n",
    "\n",
    "**Residual connections** are used to bypass the complex blocks so that the gradients can flow freely in the backward direction. Add and Norm blocks are used to merged the residual connections with other blocks.\n",
    "\n",
    "#### Decoder architecture\n",
    "\n",
    "<img  src=\"https://github.com/christianversloot/machine-learning-articles/raw/main/images/Diagram-17-627x1024.png\" width=\"500\" />\n",
    "\n",
    "The decoder is similar to the encoder but it has an additional masked multi-head self-attention layer for the shifted output in which only the words prior to the current word can be seen and the later words in the output sequence are hidden by the mask.\n",
    "\n",
    "The decoder also has a final linear+softmax layer for generating the output probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer tree:\n",
    "https://pbs.twimg.com/media/Fuz4UrZaYAAE4ZS?format=jpg&name=large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further reading\n",
    "* Reference textbook for learning more about RNN: https://www.deeplearningbook.org/contents/rnn.html\n",
    "* A wonderful blog for understanding LSTM (a special type of RNN) by Christopher Olah: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* The Illustrated Transformers blog: https://jalammar.github.io/illustrated-transformer/\n",
    "* Sebastian Raschka has provided an excellent reading list for large language models: https://sebastianraschka.com/blog/2023/llm-reading-list.html\n",
    "\n",
    "\n",
    "\n",
    "What you can do to try out Transformer-based models:\n",
    "* Check out [chatGPT](https://chat.openai.com/chat)  \n",
    "* Signup for the [waitlist](https://openai.com/waitlist/plugins) for the free access to [chatGPT plug-ins](https://openai.com/blog/chatgpt-plugins) (they [seem to have amazing potential](https://www.ted.com/talks/greg_brockman_the_inside_story_of_chatgpt_s_astonishing_potential/c) but so far in the testing phase and yet to be released widely)\n",
    "* Check out [MidJourney](https://www.midjourney.com/) for image generation (only available via Discord channel as of now)\n",
    "* Check out [Github Copilot](https://github.com/features/copilot) only if you code often (subscription starts at $10/month, no free version as of now, chatGPT does most of it for free [though you have to follow a free more steps](https://twitter.com/marktenenholtz/status/1650832042702303233?s=20))\n",
    "\n",
    "\n",
    "#### Recent applications \n",
    "Some applications for chatGPT and other LLMs (Large Language Models) are listed below. Note that this is a fast evolving field and I have not tested/tried most of the use cases mentioned below as yet.\n",
    "\n",
    "***AutoGPT or AI agents*** that complete several tasks for you using GPT-4 without any intervention from you.\n",
    "This [twitter thread by Rowan Cheung](https://twitter.com/rowancheung/status/1649798218992033793?s=20) lists a few examples\n",
    "1. Build a custom website in 3 minutes \n",
    "2. A to-do list where you list things and AutoGPT does them\n",
    "3. An agent specifically to do research for you \n",
    "4. An agent that reads about recent events and preps podcast outline \n",
    "5. Enter any goal, and your “intelligent alter ego” will do it \n",
    "6. An agent that codes for you \n",
    "7. AutoGPT inside of discord \n",
    "8. Access AutoGPT directly in your browser \n",
    "9. An AutoGPT intern that understands all tables in your data, writes SQL and sends notifications in your slack \n",
    "You can find several other threads such as [this one by Gaurav](https://twitter.com/rowancheung/status/1649798218992033793?s=20) and this latest [thread by Aakash Gupta](https://twitter.com/aakashg0/status/1649951325269155840?s=20) listing 25 use cases for Auto-GPT. \n",
    "\n",
    "\n",
    "Mckay Wrigley here [demonstrates a GPT-4 coding assistant that works via voice command](https://twitter.com/mckaywrigley/status/1644034309253394433?s=20) and plans to release it via @CodewandAI soon\n",
    "\n",
    "You can also create your own AI app/bot using this [Replit template project](https://replit.com/@MckayWrigley/Takeoff-School-Your-1st-AI-App) taking help from the [youtube demo here](https://youtu.be/JI2rmCII4fg).\n",
    "\n",
    "This [thread by Preethi Kasireddy](https://twitter.com/iam_preethi/status/1650252059243864065?s=20) neatly summarize how several companies, including [Khan Academy](https://www.khanacademy.org/khan-labs), are planning to incorporate ChatGPT for personalized and adaptive learning for students.\n",
    "\n",
    "\n",
    "#### Acknowledgement\n",
    "Some content and diagrams are taken from the online book: https://www.deeplearningbook.org/contents/rnn.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
