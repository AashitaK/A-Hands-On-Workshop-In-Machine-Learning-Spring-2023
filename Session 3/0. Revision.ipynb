{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34339a4",
   "metadata": {},
   "source": [
    "### Revision\n",
    "\n",
    "#### Logistic classifier: \n",
    "Linear decision boundary to separate the two classes.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/f663cd4f29335972950dded4d422c07aeee8af55/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a34473067737539327250684e2d636f397076315035414032782e706e67\" width=\"300\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Logistic Regression classifier </p>\n",
    "\n",
    "The classifier predicts the probability ($p$) that an observation belongs to the positive class. \n",
    "\n",
    "$$p = Prob(y=1) = sig(w_1*x_1 + w_2*x_2 + \\cdots + w_n*x_n + b) $$ \n",
    "\n",
    "where $sig$ is the sigmoid logistic function \n",
    "$$sig(t) = \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/Sigmoid-function-2.svg\" width=400 />\n",
    "\n",
    "#### Log-loss cost function (also known as cross-entropy loss function)\n",
    "\n",
    "$$ J = \\frac{1}{N} \\sum_{i=1}^N c(y, p) = - \\frac{1}{N} \\sum_{i=1}^N \\left(y \\log(p) + (1-y) \\log(1-p)\\right) $$\n",
    "\n",
    "The cost function $J(p)$ is effectively a multi-variate function $J(w_1, w_2, \\dots, w_n, b)$.\n",
    "\n",
    "#### Gradient Descent Algorithm\n",
    "\n",
    "Gradient Descent algorithm is used to iteratively update the weights using the training examples so as to minimize the cost function $J$. \n",
    "\n",
    "$$ w := w - \\alpha \\nabla J $$\n",
    "\n",
    "where $\\nabla J$ is the gradient of the cost function $J$ and $\\alpha$ is the learning rate that determines the size of steps that we take descending on the path of gradient.\n",
    "\n",
    "<img align=\"center\"  src=\"https://drive.google.com/uc?id=1K1Ki-VizvgPK88QKCQapedHItBQd8WHr\" width=\"350\" height=\"200\" />\n",
    "<p style=\"text-align: center;\"> Minimizing the cost function using gradient descent </p>\n",
    "\n",
    "#### Overfitting and Underfitting to the curve\n",
    "\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=1xXgAtcmB8pJB04OaJWR8tOxSBedfziO2\" width=\"600\" height=\"300\" />\n",
    "\n",
    "To test whether our model is overfitting or underfitting to the training set, save some examples from the datasets as a validation set  for testing model performance.\n",
    "\n",
    "|Models | Accuracy on the training set | Accuracy on the validation set | \n",
    "|---|---|---|\n",
    "| Model A | 90%| 70% |\n",
    "| Model B | 80%| 75% |\n",
    "| Model C | 70%| 65% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classifier for Titanic dataset?\n",
    "# Encoding numerical values?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
